# -*- coding: utf-8 -*-
"""DL_A3_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/125ccSLb3x6IpCsELOKP6HbRFhidrWTt_
"""

pip install datasets

import matplotlib.pyplot as plt
import datasets

# Load the dataset
dataset = datasets.load_dataset('wmt16', 'de-en')

dataset

from datasets import DatasetDict

# train_data = DatasetDict(dataset['train'][:454888])
# val_data = DatasetDict(dataset['validation'][:216])
# test_data = DatasetDict(dataset['test'][:299])

train_data = dataset['train'][:454888]
val_data = dataset['validation']
test_data = dataset['test']

from tqdm.auto import tqdm

# Get the English sentences from the training split

english_sentences = [dataset['train'][i]['translation']['en'] for i in tqdm(range(len(dataset['train'])//10))]

from tqdm.auto import tqdm

# Get the English sentences from the training split

english_sentences_valid = [dataset['validation'][i]['translation']['en'] for i in tqdm(range(len(dataset['validation'])))]

from tqdm.auto import tqdm

# Get the German sentences from the training split
german_sentences = [dataset['train'][i]['translation']['de'] for i in tqdm(range(len(dataset['train'])//10))]

from tqdm.auto import tqdm

# Get the German sentences from the training split
german_sentences_valid = [dataset['validation'][i]['translation']['de'] for i in tqdm(range(len(dataset['validation'])))]

import string

def preprocess_text(text):
    # Remove punctuation using the string library
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Lowercase the text
    text = text.lower()
    return text


eng_sentences = []
ger_sentences = []
eng_sentences_val = []
ger_sentences_val = []
eng_sentences_test = []
ger_sentences_test = []


for s1, s2 in zip(english_sentences, german_sentences):
  eng_sentences.append(preprocess_text(s1))
  ger_sentences.append(preprocess_text(s2))


for p1, p2 in zip(english_sentences_valid, german_sentences_valid):
  eng_sentences_val.append(preprocess_text(s1))
  ger_sentences_val.append(preprocess_text(s2))


# for p1, p2 in zip(english_sentences_test, german_sentences_test):
#   eng_sentences_test.append(preprocess_text(s1))
#   ger_sentences_test.append(preprocess_text(s2))

type(english_sentences)

ger_sentences

"""**Visualise the dataset**

**1. Sequence Length**
"""

# Get the sequence lengths for each language
english_lengths = [len(sentence.split()) for sentence in eng_sentences]
german_lengths = [len(sentence.split()) for sentence in ger_sentences]

# Plot the histograms
plt.hist(german_lengths, bins=50, alpha=0.5, label='German')
plt.hist(english_lengths, bins=50, alpha=0.5, label='English')
plt.xlabel('Sequence Length')
# plt.xlim(0, 150)
plt.ylabel('Count')
plt.legend()
plt.show()

"""**2. Frequency of Words**"""

import nltk
nltk.download('punkt')
from collections import Counter

# # Tokenize the German and English sentences
# german_tokens = [token for sentence in german_sentences for token in nltk.word_tokenize(sentence)]
# english_tokens = [token for sentence in english_sentences for token in nltk.word_tokenize(sentence)]


# Tokenize the German sentences
german_tokens = []
for sentence in ger_sentences:
    for token in nltk.word_tokenize(sentence):
        german_tokens.append(token)

# Tokenize the English sentences
english_tokens = []
for sentence in eng_sentences:
    for token in nltk.word_tokenize(sentence):
        english_tokens.append(token)

# Get the frequency distribution for each language
german_freq = Counter(german_tokens)
english_freq = Counter(english_tokens)

# Get the top 20 most common words in each language
german_top_words = german_freq.most_common(20)
english_top_words = english_freq.most_common(20)

# Plot the bar charts
plt.bar(range(len(german_top_words)), [freq for word, freq in german_top_words], tick_label=[word for word, freq in german_top_words])
plt.xticks(rotation=90)
plt.title('German Word Frequency Distribution')
plt.show()


plt.bar(range(len(english_top_words)), [freq for word, freq in english_top_words], tick_label=[word for word, freq in english_top_words])
plt.xticks(rotation=90)
plt.title('English Word Frequency Distribution')
plt.show()

german_freq

english_freq

"""**3. Mean Token Length**"""

# Calculate the mean token length for each language
german_mean_length = sum([len(token) for token in german_tokens]) / len(german_tokens)
english_mean_length = sum([len(token) for token in english_tokens]) / len(english_tokens)

# Print the results
print(f"German Mean Token Length: {german_mean_length:.2f}")
print(f"English Mean Token Length: {english_mean_length:.2f}")

import matplotlib.pyplot as plt

# Define the data
languages = ['German', 'English']
mean_lengths = [german_mean_length, english_mean_length]

# Set up the plot
fig, ax = plt.subplots()
ax.bar(languages, mean_lengths)

# Add labels and title
ax.set_xlabel('Language')
ax.set_ylabel('Mean Token Length')
ax.set_title('Mean Token Length by Language')

# Show the plot
plt.show()

"""**4. Word Cloud**"""

import nltk
nltk.download('stopwords')

from wordcloud import WordCloud

# Generate the German word cloud
german_wordcloud = WordCloud(width=800, height=800, background_color='white', stopwords=nltk.corpus.stopwords.words('german'), min_font_size=10).generate_from_frequencies(german_freq)
plt.imshow(german_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Generate the English word cloud
english_wordcloud = WordCloud(width=800, height=800, background_color='white', stopwords=nltk.corpus.stopwords.words('english'), min_font_size=10).generate_from_frequencies(english_freq)
plt.imshow(english_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

import nltk
nltk.download('averaged_perceptron_tagger')

"""**5. POS Tag**"""

import nltk
import matplotlib.pyplot as plt

# # Tokenize the German and English sentences
# german_tokens = nltk.word_tokenize('\n'.join(dataset['train']['translation']['de']))
# english_tokens = nltk.word_tokenize('\n'.join(dataset['train']['translation']['en']))

# Get the POS tags and frequencies for German and English
import nltk
import matplotlib.pyplot as plt

# Define a function to get POS tags and frequencies for a list of tokens
def get_pos_freq(tokens):
    pos_tags = nltk.pos_tag(tokens)
    pos_freq = nltk.FreqDist(tag for (word,tag) in pos_tags)
    return pos_freq

# Get the POS tags and frequencies for German and English
german_pos_freq = get_pos_freq(german_tokens)
english_pos_freq = get_pos_freq(english_tokens)

# Plot the top 10 POS tags for German and English
plt.bar(range(10), [freq[1] for freq in german_pos_freq.most_common(10)], tick_label=[freq[0] for freq in german_pos_freq.most_common(10)], alpha=0.5, label='German')
plt.bar(range(10), [freq[1] for freq in english_pos_freq.most_common(10)], tick_label=[freq[0] for freq in english_pos_freq.most_common(10)], alpha=0.5, label='English')
plt.xticks(rotation=90)
plt.xlabel('POS tag')
plt.ylabel('Frequency')
plt.legend()
plt.show()